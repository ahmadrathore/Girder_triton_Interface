{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "vmtouch is already the newest version (1.3.1-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.8/dist-packages (1.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tritonclient.http as tritonhttpclient\n",
    "import tritonclient.grpc as tritongrpcclient\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import gevent.ssl\n",
    "import tritonclient.grpc as grpcclient\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import InferenceServerException\n",
    "import easydict\n",
    "import argparse\n",
    "import numpy as np\n",
    "import sys\n",
    "from builtins import range\n",
    "from ctypes import *\n",
    "import tritonclient.grpc as grpcclient\n",
    "from tritonclient import utils\n",
    "import tritonclient.utils.shared_memory as shm\n",
    "import tritonclient.http as tritonhttpclient\n",
    "import subprocess\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from appdirs import user_cache_dir\n",
    "from numpy import load\n",
    "from appdirs import user_cache_dir\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from tritonclient.utils import triton_to_np_dtype\n",
    "!apt-get install vmtouch\n",
    "!pip install appdirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGirder:\n",
    "    first = 0\n",
    "    second = 0\n",
    "    answer = 0\n",
    "\n",
    "    # parameterized constructor\n",
    "    def __init__(self, model_name, output_saved_model_dir, grpc_url, verbose, \n",
    "                 model_connection_url, server_health, configuration, batch_size,\n",
    "                 input_name,output_name,model_version,url,\n",
    "                 data_path, batch):\n",
    "        self.model_name = model_name\n",
    "        self.grpc_url = grpc_url\n",
    "        self.verbose = verbose\n",
    "        self.model_connection_url=model_connection_url\n",
    "        self.server_health = server_health\n",
    "        self.configuration=configuration\n",
    "        self.output_saved_model_dir= output_saved_model_dir\n",
    "        self.batch_size=batch_size\n",
    "        self.input_name = input_name\n",
    "        self.output_name = output_name\n",
    "        self.output_name_1='output_0'\n",
    "        self.model_version = model_version\n",
    "        self.url = url\n",
    "        self.data_path=data_path\n",
    "        self.batch_copy= batch\n",
    "        self.boolean = 'false'\n",
    "\n",
    "    def protocol(self):\n",
    "        self.triton_grpc_client = tritongrpcclient.InferenceServerClient(url=self.grpc_url, verbose=self.verbose)\n",
    "        self.answer = self.first + self.second\n",
    "    def model(self):\n",
    "        output_saved_model_dir= self.output_saved_model_dir\n",
    "    def model_connection_check(self):\n",
    "        import requests\n",
    "        res = requests.get('http://'+ str(self.model_connection_url))\n",
    "        if (res.status_code == 200):\n",
    "          print(\"Model Connection esteblished\")\n",
    "        else:\n",
    "          print(\"Model Connection not esteblished\")      \n",
    "\n",
    "    def health(self):\n",
    "        import requests\n",
    "        server_health= subprocess.check_output(['curl', '-v',self.server_health])\n",
    "        res = requests.get('http://' + str(self.server_health))\n",
    "        if (res.status_code == 200):\n",
    "            print(\"Server health esteblished\")\n",
    "        else:\n",
    "            print(\"Server health not esteblished\")    \n",
    "    \n",
    "    def instantiate_triton_client(self):\n",
    "        self.triton_grpc_client = tritongrpcclient.InferenceServerClient(url=self.grpc_url, verbose=self.verbose)\n",
    "        model_metadata = self.triton_grpc_client.get_model_metadata(model_name=self.model_name, model_version=self.model_version)\n",
    "        model_config = self.triton_grpc_client.get_model_config(model_name=self.model_name, model_version=self.model_version)\n",
    "\n",
    "    def batch(self):  \n",
    "        from numpy import load\n",
    "        # load array\n",
    "        self.batch = load(self.data_path)\n",
    "\n",
    "    def infer(self):\n",
    "        from numpy import asarray\n",
    "        from numpy import save\n",
    "        import numpy as np\n",
    "        # data loaded without using cache memory\n",
    "        self.input1 = tritongrpcclient.InferInput(input_name, self.batch_copy.shape, 'FP16')\n",
    "        self.input1.set_data_from_numpy(self.batch_copy)\n",
    "        self.output1 = tritongrpcclient.InferRequestedOutput(self.output_name_1)\n",
    "\n",
    "        # Infer from cache \n",
    "        self.input0 = tritongrpcclient.InferInput(input_name, self.batch.shape, 'FP16')\n",
    "\n",
    "        self.input0.set_data_from_numpy(load(self.data_path))\n",
    "        self.output = tritongrpcclient.InferRequestedOutput(self.output_name)\n",
    "        \n",
    "    def throughput(self):\n",
    "        from functools import partial\n",
    "        import sys\n",
    "\n",
    "        if sys.version_info >= (3, 0):\n",
    "            import queue\n",
    "        else:\n",
    "            import Queue as queue\n",
    "        class UserData:\n",
    "            def __init__(self):\n",
    "                self._completed_requests = queue.Queue()\n",
    "        def __init__(self):\n",
    "            self._completed_requests = queue.Queue()\n",
    "        def completion_callback(user_data, result, error):\n",
    "            # passing error raise and handling out\n",
    "            user_data._completed_requests.put((result, error))\n",
    "        user_data = UserData()\n",
    "        \n",
    "        models = [model_name]\n",
    "\n",
    "        async_requests = []\n",
    "        responses = []\n",
    "        request_count = 1000\n",
    "\n",
    "        for i in tqdm(range(50)):\n",
    "            async_requests.append(self.triton_grpc_client.async_infer(model_name, inputs=[self.input1],\n",
    "                                                     callback=partial(completion_callback, user_data),\n",
    "                                                               outputs=[self.output1]))\n",
    "\n",
    "        print(\"Throughput with cache memory\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in tqdm(range(request_count)):\n",
    "                async_requests.append(self.triton_grpc_client.async_infer(model_name, inputs=[self.input0],\n",
    "                                                     callback=partial(completion_callback, user_data),\n",
    "                                                         outputs=[self.output]))\n",
    "        sent_count = 1\n",
    "        processed_count = 0\n",
    "        while processed_count < sent_count:\n",
    "            (results, error) = user_data._completed_requests.get()\n",
    "            processed_count += 1\n",
    "            if error is not None:\n",
    "                print(\"inference failed: \" + str(error))\n",
    "                sys.exit(1)\n",
    "            responses.append(user_data) \n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "\n",
    "        print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "        print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already in Cache Memory\n",
      "Model Connection esteblished\n",
      "Server health esteblished\n"
     ]
    }
   ],
   "source": [
    "tfr_files_path = '/tf/notebooks/Course-ece495/train'\n",
    "npy_files_path = user_cache_dir(\"\")\n",
    "batch_size=2048\n",
    "output_saved_model_dir = 'models/simple-trt-model-FP16/1/model.savedmodel'\n",
    "model_connection_url = 'localhost:8000/v2/models/simple-trt-model-FP16/config'\n",
    "server_health = 'localhost:8000/v2/health/ready'\n",
    "input_name = 'input_0'\n",
    "input_shape = (batch_size,1024)\n",
    "input_dtype = 'FP16'\n",
    "output_name = 'output_0'\n",
    "model_name = 'simple-trt-model-FP16'\n",
    "url = 'localhost:8000'\n",
    "model_version = '1'\n",
    "\n",
    "configuration = \"\"\"\n",
    "name: \"simple-trt-model-FP16\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size: 2048\n",
    "\n",
    "input [\n",
    " {\n",
    "    name: \"input_0\"\n",
    "    data_type: TYPE_FP16\n",
    "    dims: [ 1024 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output_0\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1 ]\n",
    "  }\n",
    "instance_group [\n",
    "    {\n",
    "      count: 16\n",
    "      kind: KIND_GPU\n",
    "      gpus: [ 0,1]\n",
    "    },\n",
    "    {\n",
    "      count: 16\n",
    "      kind: KIND_GPU\n",
    "      gpus: [2,3]\n",
    "    },\n",
    "    {\n",
    "      count: 4\n",
    "      kind: KIND_CPU\n",
    "    }\n",
    "  ]\n",
    "dynamic_batching {\n",
    "    \n",
    "    max_queue_delay_microseconds: 200\n",
    "  }\n",
    "\"\"\"\n",
    "with open('models/simple-trt-model-FP16/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)\n",
    "    \n",
    "# Put the files in cache memory for fast access\n",
    "def files_cache(s, pat=re.compile('100%')):\n",
    "    if pat.search(s):\n",
    "        print(\"Files already in Cache Memory\")\n",
    "    else:\n",
    "        print(\"Adding Files in Cache Memory\")\n",
    "        subprocess.check_output(['vmtouch','-t',npy_files_path])        \n",
    "\n",
    "#create numpy data\n",
    "class iterator(object):\n",
    "\n",
    "    def __init__(self, B=batch_size, D=1024):\n",
    "        self.B = B # batch size\n",
    "        self.D = D # dimension\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        output = np.float16(np.random.uniform(size=(self.B, self.D)))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def batch(self):     \n",
    "        \n",
    "        self.batch = (iterator(self.B))\n",
    "        i = iter(self.batch)\n",
    "        self.batch=next(i)\n",
    "        return self.batch\n",
    "\n",
    "# Generate a numpy array as npy file\n",
    "obj = iterator()\n",
    "batch =  obj.batch()\n",
    "save(npy_files_path + '/batch.npy', batch)\n",
    "\n",
    "# Adding numpy batch to cache memory\n",
    "# for .tfr files change to npy_files_path = tfr_files_path\n",
    "data_path = npy_files_path + '/batch.npy' # change to tfr_files_path for .tfr files\n",
    "result= subprocess.check_output(['vmtouch',npy_files_path])\n",
    "files_cache(str(result))\n",
    "\n",
    "# creating object of the Girdir class\n",
    "# this will invoke parameterized constructor\n",
    "obj = IGirder(model_name,output_saved_model_dir,\n",
    "             'localhost:8001',False,\n",
    "             model_connection_url,server_health,\n",
    "            configuration, batch_size,input_name,output_name,\n",
    "            model_version,url,\n",
    "            data_path,batch)\n",
    " \n",
    "# perform Addition\n",
    "obj.instantiate_triton_client()\n",
    "# perform model connection\n",
    "obj.model_connection_check()\n",
    "#\n",
    "obj.health()\n",
    "obj.batch()\n",
    "obj.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 192.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput with cache memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:04<00:00, 231.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency: ~0.004327610015869141 seconds\n",
      "Average Throughput: ~473240.42427346297 examples / second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "obj.throughput()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
